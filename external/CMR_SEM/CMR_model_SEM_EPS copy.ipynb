{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strength of drift\n",
    "# scale_drift = 0.4  # scaling parameter (not used if inputting manual drift)\n",
    "\n",
    "# strength of semantic + episodic route\n",
    "sem = 0.5\n",
    "episodic = 0.5\n",
    "\n",
    "# normalize\n",
    "sem_weight = sem / (episodic + sem)\n",
    "episodic_weight = episodic / (episodic + sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reward sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample (1-indexed to match R behavior, then convert to 0-indexed)\n",
    "pres_indices = np.random.permutation(10) + 1\n",
    "\n",
    "# high RPE within sequence (first outcome is \"reward expectation\" at 0)\n",
    "# sequence = np.array([0, 54, 57, 56, 53, 55, 7, 5, 4, 6, 3])  # initial expectation is 0\n",
    "\n",
    "# primacy (first outcome is \"reward expectation\" at 50)\n",
    "sequence = np.array([50, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dynamic drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic beta\n",
    "# absPE = np.zeros((len(pres_indices), 1))\n",
    "\n",
    "# take absolute difference of previous reward with current reward\n",
    "# (first \"prediction error\" is reward - initial expectation)\n",
    "# for seq in range(len(sequence) - 1):\n",
    "#     absPE[seq] = abs(sequence[seq] - sequence[seq + 1])\n",
    "\n",
    "# take absolute value of z-scored absPE and multiply by scaling parameter\n",
    "# B_encD = np.abs(zscore(absPE)) * scale_drift\n",
    "\n",
    "# B_encD = np.where(B_encD > 1, 1, B_encD)  # if it's over 1, make it 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual drift (comment out if using above or stable drift)\n",
    "B_encD = np.array([1, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"semantic\" matrix\n",
    "# each item has an activation of 1 (and to no other units)\n",
    "sem_mat = np.eye(len(pres_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original parameters (polyn, norman & kahana, 2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PARAMETERS\n",
    "\n",
    "# for creating the network\n",
    "gamma_fc = 0.581  # relative strength of pre-existing associations on connections feature-context\n",
    "eye_fc = 1 - gamma_fc  # if items are rep as orthonormal vectors, identity matrix (\"eye\")\n",
    "eye_cf = 0\n",
    "\n",
    "# during encoding\n",
    "B_enc = 0.745  # vector of context integration rate at encoding, dynamic alternative above\n",
    "lrate_fc_enc = gamma_fc  # feature-to-context during encoding\n",
    "lrate_cf_enc = 1  # context-to-feature during encoding\n",
    "\n",
    "# during recall\n",
    "B_rec = 0.36  # vector of context integration at recall\n",
    "lrate_fc_rec = 0  # feature-to-context during recall\n",
    "lrate_cf_rec = 0  # context-to-feature during recall\n",
    "thresh = 1  # threshold for an accumulating element to win the decision competition (fixed at 1)\n",
    "rec_time = 90000  # max recall process (interpreted as 90 seconds)\n",
    "dt = 100  # time constant on decision process\n",
    "L = 0.375  # lateral inhibition between units in decision competition\n",
    "K = 0.091  # decay rate for the accumulating elements in decision competition\n",
    "eta = 0.3699  # standard deviation of gaussian noise term in decision competition\n",
    "tau = 413  # time constant in decision competition\n",
    "\n",
    "n_sims = 1000  # number of simulations\n",
    "recall_sims = np.zeros((len(pres_indices), n_sims))\n",
    "times_sims = np.zeros((len(pres_indices), n_sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sims in range(n_sims):\n",
    "    \n",
    "    # NETWORK\n",
    "    \n",
    "    # initialize the features and context layers\n",
    "    net_f = np.zeros((len(pres_indices), 1))\n",
    "    net_c = np.zeros((len(pres_indices), 1))\n",
    "    \n",
    "    # learning rate matrices\n",
    "    net_lrate_fc = np.zeros((len(pres_indices), len(pres_indices)))\n",
    "    net_lrate_cf = np.zeros((len(pres_indices), len(pres_indices)))\n",
    "    \n",
    "    # the lrate matrices\n",
    "    net_lrate_fc_enc = np.full((len(pres_indices), len(pres_indices)), lrate_fc_enc)\n",
    "    net_lrate_cf_enc = np.full((len(pres_indices), len(pres_indices)), lrate_cf_enc)\n",
    "    net_lrate_fc_rec = np.full((len(pres_indices), len(pres_indices)), lrate_fc_rec)\n",
    "    net_lrate_cf_rec = np.full((len(pres_indices), len(pres_indices)), lrate_cf_rec)\n",
    "    \n",
    "    net_w_fc = np.eye(len(net_c)) * eye_fc  # m_fc eye() creates identity matrices\n",
    "    net_w_cf = np.eye(len(net_f)) * eye_cf  # m_cf zero\n",
    "    net_weights = np.zeros((len(pres_indices), len(pres_indices)))\n",
    "    \n",
    "    # ENCODING\n",
    "    \n",
    "    net_idx = np.arange(len(pres_indices))\n",
    "    \n",
    "    for item in range(len(pres_indices)):\n",
    "        \n",
    "        # present item\n",
    "        feature_idx = pres_indices[item] - 1  # Convert to 0-indexed\n",
    "        \n",
    "        # activates the indexed feature (each item activates one element)\n",
    "        net_f = np.zeros((len(pres_indices), 1))\n",
    "        net_f[feature_idx] = 1\n",
    "        \n",
    "        # update context representations\n",
    "        net_c_in = net_w_fc @ net_f\n",
    "        \n",
    "        # normalize vector\n",
    "        vec = net_c_in\n",
    "        denom_vec = np.sqrt(vec.T @ vec)[0, 0]\n",
    "        norm_vec = vec / denom_vec\n",
    "        net_c_in = norm_vec\n",
    "        \n",
    "        # advance context\n",
    "        c_in = net_c_in\n",
    "        c = net_c\n",
    "        \n",
    "        # set dynamic or stable drift\n",
    "        B = B_encD[item]  # beta at encoding for dynamic\n",
    "        # B = B_enc  # beta at encoding if stable\n",
    "        \n",
    "        dot_product = (c.T @ c_in)[0, 0]\n",
    "        rho = np.sqrt(1 + (B**2) * ((dot_product**2) - 1)) - B * dot_product\n",
    "        updated_c = rho * c + B * c_in\n",
    "        net_c = updated_c\n",
    "        \n",
    "        # determine current learning rate\n",
    "        lrate_fc = net_lrate_fc_enc\n",
    "        lrate_cf = net_lrate_cf_enc\n",
    "        \n",
    "        # update weights\n",
    "        \n",
    "        # w_fc\n",
    "        delta = (net_c @ net_f.T) * lrate_fc\n",
    "        net_w_fc = net_w_fc + delta\n",
    "        \n",
    "        # w_cf\n",
    "        delta = (net_f @ net_c.T) * lrate_cf\n",
    "        net_w_cf = net_w_cf + delta\n",
    "    \n",
    "    # RECALL\n",
    "    \n",
    "    # set up\n",
    "    recalls = np.zeros((len(pres_indices), 1))\n",
    "    times = np.zeros((len(pres_indices), 1))\n",
    "    \n",
    "    rec_time_local = 90000\n",
    "    time_passed = 0\n",
    "    recall_count = 0\n",
    "    \n",
    "    retrieved = np.zeros((len(pres_indices), 1), dtype=bool)\n",
    "    thresholds = np.ones((len(pres_indices), 1))\n",
    "    \n",
    "    # semantic + episodic routes\n",
    "    net_weights = episodic_weight * net_w_cf + sem_weight * sem_mat\n",
    "    \n",
    "    # GO!!!\n",
    "    \n",
    "    while time_passed < rec_time_local:\n",
    "        \n",
    "        # input to the feature layer, from last context cue\n",
    "        f_in = net_weights @ net_c\n",
    "        \n",
    "        # set max number of cycles\n",
    "        max_cycles = int((rec_time_local - time_passed) / dt)\n",
    "        \n",
    "        # for noise error standard deviation\n",
    "        dt_tau = dt / tau\n",
    "        sq_dt_tau = np.sqrt(dt_tau)\n",
    "        \n",
    "        # noise matrix\n",
    "        noise = np.random.normal(0, eta * sq_dt_tau, (len(pres_indices), max_cycles))\n",
    "        eyeI = ~np.eye(len(pres_indices), dtype=bool)\n",
    "        lmat = eyeI.astype(float) * L\n",
    "        \n",
    "        ncycles = noise.shape[1]\n",
    "        inds = np.arange(len(pres_indices))\n",
    "        \n",
    "        crossed = 0\n",
    "        \n",
    "        x = np.zeros((len(pres_indices), 1))\n",
    "        \n",
    "        K_array = np.ones((len(pres_indices), 1)) * K\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        # ACCUMULATORS CYCLING\n",
    "        while i < ncycles and crossed == 0:\n",
    "            \n",
    "            # the lateral inhibition felt by each unit\n",
    "            lx = lmat @ x\n",
    "            \n",
    "            # the activity leaking from each unit\n",
    "            kx = K_array * x\n",
    "            \n",
    "            # change in each accumulator\n",
    "            x = x + ((f_in - kx - lx) * dt_tau + noise[:, i:i+1])\n",
    "            x[x < 0] = 0\n",
    "            \n",
    "            # reset retrieved values, allow them to compete but prevent from accumulating\n",
    "            reset_these = retrieved & (x >= thresholds)\n",
    "            x[reset_these] = 0.95 * thresholds[reset_these]\n",
    "            \n",
    "            # retrieved items cannot be repeated\n",
    "            retrievable = ~retrieved\n",
    "            \n",
    "            # determine whether any items have crossed thresholds\n",
    "            crossed = 0\n",
    "            if np.any(x[retrievable] >= thresholds[retrievable]):\n",
    "                crossed = 1\n",
    "                temp_win = x[retrievable] >= thresholds[retrievable]\n",
    "                temp_ind = inds[retrievable.flatten()]\n",
    "                winners = temp_ind[temp_win.flatten()]\n",
    "                \n",
    "                # if there is a tie, random tiebreak\n",
    "                if len(winners) > 1:\n",
    "                    winners = np.array([np.random.choice(winners)])\n",
    "                \n",
    "                winner_position = np.where(pres_indices - 1 == winners[0])[0][0]\n",
    "            \n",
    "            i = i + 1\n",
    "        \n",
    "        # calculate the amount of elapsed time\n",
    "        time = i * dt\n",
    "        \n",
    "        time_passed = time_passed + time\n",
    "        \n",
    "        # reactivate item if there has been a retrieval\n",
    "        if crossed == 1:\n",
    "            \n",
    "            # activate the retrieved feature\n",
    "            net_f = np.zeros((len(pres_indices), 1))\n",
    "            net_f[winners[0]] = 1\n",
    "            \n",
    "            # update context representations\n",
    "            net_c_in = net_w_fc @ net_f\n",
    "            \n",
    "            # normalize vector\n",
    "            vec = net_c_in\n",
    "            denom_vec = np.sqrt(vec.T @ vec)[0, 0]\n",
    "            norm_vec = vec / denom_vec\n",
    "            net_c_in = norm_vec\n",
    "            \n",
    "            # advance context\n",
    "            c_in = net_c_in\n",
    "            c = net_c\n",
    "            B = B_rec  # beta at retrieval\n",
    "            \n",
    "            dot_product = (c.T @ c_in)[0, 0]\n",
    "            rho = np.sqrt(1 + (B**2) * ((dot_product**2) - 1)) - B * dot_product\n",
    "            updated_c = rho * c + B * c_in\n",
    "            net_c = updated_c\n",
    "            \n",
    "            # determine current learning rate\n",
    "            lrate_fc = net_lrate_fc_rec\n",
    "            lrate_cf = net_lrate_cf_rec\n",
    "            \n",
    "            # w_fc\n",
    "            delta = (net_c @ net_f.T) * lrate_fc\n",
    "            net_w_fc = net_w_fc + delta\n",
    "            \n",
    "            # w_cf\n",
    "            delta = (net_f @ net_c.T) * lrate_cf\n",
    "            net_w_cf = net_w_cf + delta\n",
    "            \n",
    "            # record data\n",
    "            recall_count = recall_count + 1\n",
    "            recalls[recall_count - 1, 0] = winner_position + 1  # Convert back to 1-indexed\n",
    "            times[recall_count - 1, 0] = time_passed\n",
    "            \n",
    "            # update retrieved vector\n",
    "            retrieved[winners[0]] = True\n",
    "    \n",
    "    recall_sims[:, sims] = recalls.flatten()\n",
    "    times_sims[:, sims] = times.flatten()\n",
    "\n",
    "print(f\"Model run complete. Simulated {n_sims} trials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# serial position curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total proportion of recall given the serial position\n",
    "position = pd.DataFrame({'position': np.arange(1, len(pres_indices) + 1)})\n",
    "\n",
    "numSums = np.zeros(len(pres_indices))\n",
    "\n",
    "for numSum in range(len(numSums)):\n",
    "    numSums[numSum] = np.sum(recall_sims == (numSum + 1))\n",
    "\n",
    "recall = numSums / n_sims\n",
    "\n",
    "prop_recall = pd.DataFrame({\n",
    "    'position': position['position'],\n",
    "    'recall': recall\n",
    "})\n",
    "\n",
    "print(prop_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recall success as a function of serial position\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(prop_recall['position'], prop_recall['recall'], 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('serial position', fontsize=15)\n",
    "plt.ylabel('probability of recall', fontsize=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first recall probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the proportion of \"first recall\" items as a function of serial position\n",
    "first_recall = recall_sims[0, :]\n",
    "first_recall = first_recall[first_recall > 0]  # Filter out zeros\n",
    "\n",
    "# Count frequencies\n",
    "unique, counts = np.unique(first_recall, return_counts=True)\n",
    "first_recall_table = pd.DataFrame({\n",
    "    'position': unique.astype(int),\n",
    "    'freq': counts,\n",
    "    'prop': counts / n_sims\n",
    "})\n",
    "\n",
    "print(first_recall_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first recall proportion as a function of serial position\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(first_recall_table['position'], first_recall_table['prop'], 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('serial position', fontsize=15)\n",
    "plt.ylabel('probability of first recall', fontsize=15)\n",
    "plt.ylim(0, 0.8)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conditional response probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poss_outcomes = np.arange(-9, 10)\n",
    "poss_outcomes = np.delete(poss_outcomes, 9)  # Remove the element at index 9 (value 0)\n",
    "\n",
    "# create matrix of actual transitions\n",
    "trans_sims = np.zeros((len(pres_indices), n_sims))\n",
    "\n",
    "for subj in range(n_sims):\n",
    "    currentSub = recall_sims[:, subj]\n",
    "    \n",
    "    for trial in range(9):\n",
    "        if currentSub[trial + 1] > 0:\n",
    "            trans_sims[trial, subj] = currentSub[trial + 1] - currentSub[trial]\n",
    "        else:\n",
    "            trans_sims[trial, subj] = 0\n",
    "\n",
    "# create matrix of all possible transitions\n",
    "possTransFrame = []\n",
    "\n",
    "for subj in range(n_sims):\n",
    "    currentSub = recall_sims[:, subj]\n",
    "    possTrans_sims = np.zeros((10, 9))\n",
    "    \n",
    "    for trial in range(9):\n",
    "        if currentSub[trial + 1] > 0:\n",
    "            currentTrial = currentSub[trial]\n",
    "            itemTally = currentSub[0:trial + 1]\n",
    "            possPositions = pres_indices[~np.isin(pres_indices, itemTally)]\n",
    "            possTransitions = possPositions - currentTrial\n",
    "            \n",
    "            for poss in range(len(possTransitions)):\n",
    "                possTrans_sims[trial, poss] = possTransitions[poss]\n",
    "    \n",
    "    possTransFrame.append(possTrans_sims)\n",
    "\n",
    "possTransFrame = np.vstack(possTransFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CRP\n",
    "actual_transitions = trans_sims[trans_sims != 0]\n",
    "possible_transitions = possTransFrame[possTransFrame != 0]\n",
    "\n",
    "# Count frequencies\n",
    "tab_a_t_unique, tab_a_t_counts = np.unique(actual_transitions, return_counts=True)\n",
    "tab_p_t_unique, tab_p_t_counts = np.unique(possible_transitions, return_counts=True)\n",
    "\n",
    "tab_a_t = pd.DataFrame({\n",
    "    'actual_transitions': tab_a_t_unique,\n",
    "    'Freq': tab_a_t_counts\n",
    "})\n",
    "\n",
    "tab_p_t = pd.DataFrame({\n",
    "    'possible_transitions': tab_p_t_unique,\n",
    "    'Freq': tab_p_t_counts\n",
    "})\n",
    "\n",
    "# Filter\n",
    "tab_a_t = tab_a_t[(tab_a_t['actual_transitions'] < 6) & (tab_a_t['actual_transitions'] > -6)]\n",
    "tab_p_t = tab_p_t[(tab_p_t['possible_transitions'] < 6) & (tab_p_t['possible_transitions'] > -6)]\n",
    "\n",
    "# Calculate CRP\n",
    "crp = tab_a_t['Freq'].values / tab_p_t['Freq'].values\n",
    "\n",
    "crps = pd.DataFrame({\n",
    "    'transitions': tab_a_t['actual_transitions'].values,\n",
    "    'crp': crp\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot CRP\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(crps['transitions'], crps['crp'], 'o-', linewidth=2, markersize=8)\n",
    "\n",
    "# Add white segments to mask the gap at lag 0 (between -1 and 1)\n",
    "mask_indices = (crps['transitions'] == -1) | (crps['transitions'] == 1)\n",
    "if mask_indices.sum() == 2:\n",
    "    idx_neg1 = crps[crps['transitions'] == -1].index[0]\n",
    "    idx_pos1 = crps[crps['transitions'] == 1].index[0]\n",
    "    plt.plot([-1, 1], [crps.loc[idx_neg1, 'crp'], crps.loc[idx_pos1, 'crp']], \n",
    "             'w-', linewidth=3, zorder=10)\n",
    "\n",
    "plt.xlabel('lag', fontsize=15)\n",
    "plt.ylabel('conditional response probability', fontsize=15)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-organize weight matrices for plotting (so as to view them by serial position)\n",
    "net_w_fc_inorder = net_w_fc[:, pres_indices - 1]  # Convert to 0-indexed\n",
    "net_w_cf_inorder = net_w_cf[pres_indices - 1, :]  # Convert to 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up weight matrices plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Feature to context weight matrix\n",
    "im1 = axes[0].imshow(net_w_fc_inorder, cmap='YlOrRd', aspect='auto')\n",
    "axes[0].set_title('feature to context weight matrix', fontsize=16)\n",
    "axes[0].set_xlabel('serial position', fontsize=14)\n",
    "axes[0].set_ylabel('', fontsize=14)\n",
    "axes[0].set_xticks(np.arange(len(pres_indices)))\n",
    "axes[0].set_xticklabels(np.arange(1, len(pres_indices) + 1))\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Context to feature weight matrix\n",
    "im2 = axes[1].imshow(net_w_cf_inorder.T, cmap='Reds', aspect='auto')\n",
    "axes[1].set_title('context to feature weight matrix', fontsize=16)\n",
    "axes[1].set_xlabel('serial position', fontsize=14)\n",
    "axes[1].set_ylabel('', fontsize=14)\n",
    "axes[1].set_xticks(np.arange(len(pres_indices)))\n",
    "axes[1].set_xticklabels(np.arange(1, len(pres_indices) + 1))\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up correlation plots\n",
    "corr_fc = np.corrcoef(net_w_fc_inorder.T)\n",
    "corr_cf = np.corrcoef(net_w_cf_inorder)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Feature to context correlation matrix\n",
    "im1 = axes[0].imshow(corr_fc, cmap='YlOrRd', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_title('feature to context weight matrix', fontsize=16)\n",
    "axes[0].set_xlabel('serial position', fontsize=14)\n",
    "axes[0].set_ylabel('', fontsize=14)\n",
    "axes[0].set_xticks(np.arange(len(pres_indices)))\n",
    "axes[0].set_xticklabels(np.arange(1, len(pres_indices) + 1))\n",
    "axes[0].set_yticks(np.arange(len(pres_indices)))\n",
    "axes[0].set_yticklabels(np.arange(1, len(pres_indices) + 1))\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Context to feature correlation matrix\n",
    "im2 = axes[1].imshow(corr_cf, cmap='YlOrRd', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1].set_title('context to feature weight matrix', fontsize=16)\n",
    "axes[1].set_xlabel('serial position', fontsize=14)\n",
    "axes[1].set_ylabel('', fontsize=14)\n",
    "axes[1].set_xticks(np.arange(len(pres_indices)))\n",
    "axes[1].set_xticklabels(np.arange(1, len(pres_indices) + 1))\n",
    "axes[1].set_yticks(np.arange(len(pres_indices)))\n",
    "axes[1].set_yticklabels(np.arange(1, len(pres_indices) + 1))\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
